{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce3c6d6-43f5-42a6-9666-23632615ac48",
   "metadata": {},
   "source": [
    "# Medical Query Prototype \n",
    "\n",
    "## Description:\n",
    "As per the document: \"A prototype tool that allows users to query about medical drugs, search online medical news or research study sources via an API or any search strategies, index the relevant results, and use a Large Language Model (LLM) to summarize the information to answer the user's query.\"\n",
    "\n",
    "## Discussion: \n",
    "Suppose that we take medical papers and either (1) web-scrap relevant data (2) similarly scrap from a pdf, it would be preferable that our pipeline be a Question Answering system that can not only answer within a good accuracy but tell us when it cannot answer the query. For instance, if a common question is that of \"What are the main competitor drugs of [name] and what are their latest news now?\" If no such token allows for an answer we would want it to tell us so, essentially reading the paper for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313bf2e-5064-4b9d-b254-32941bfa2e07",
   "metadata": {},
   "source": [
    "## Installation Instructions. (My machine is Ubuntu (Debian))\n",
    "1. <code> pip install llama-index <code>\n",
    "2. <code> pip install llama-index-readers-file <code>\n",
    "3. <code> pip install llama-index-readers-web <code>\n",
    "4. <code> pip install llama-index-embeddings-openai <code> OR <code> pip install llama-index-embeddings-huggingface <code>\n",
    "5. <code> pip install llama-index-llms-openai <code>\n",
    "\n",
    "UI interface reqs: \n",
    "\n",
    "<code> pip install gradio <code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0438cac4-86ed-4173-a965-68cbbb8c90f4",
   "metadata": {},
   "source": [
    "# API Integration for News Search/API/PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2bc4a6-3f60-4de2-8b8e-8b2482cf5e75",
   "metadata": {},
   "source": [
    "## PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb56e37-8d8b-4168-8085-3969733daa4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define our documents, LlamaIndex makes this particularly easy with it's Data Loaders on LlamaHub object and open source use.\n",
    "\n",
    "from llama_index.readers.file import PDFReader #We have a Data Loader for our PDF papers\n",
    "\n",
    "#PDF Loader\n",
    "pdf_loader = PDFReader()\n",
    "\n",
    "#Document Objects \n",
    "pdf_documents = pdf_loader.load_data(\"path/to/pdf/paper.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87028d6-43d6-4127-bd72-fe16bb8f5467",
   "metadata": {},
   "source": [
    "## Web Loader (News/Web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dbd265-6c2e-4d46-95b1-5dbc78f0baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define our documents, LlamaIndex makes this particularly easy with it's Data Loaders on LlamaHub object and open source use.\n",
    "## This sort of web loader pattern can be extend towards News Sources instead of having a specific API for it. \n",
    "\n",
    "from llama_index.readers.web import BeautifulSoupWebReader #We have a Data Loader for a website (Note: This can potentially be difficult to do initially\n",
    "#and would require some other custom changes for dynamic sites, as well as sites that require logins and so forth, but this is a simple example.)\n",
    "\n",
    "#Web Loader\n",
    "web_loader = BeautifulSoupWebReader()\n",
    "\n",
    "#Document Objects \n",
    "web_documents = web_loader.load_data(urls=[URL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffa7e7c-3c77-4642-bdf9-840d482858e5",
   "metadata": {},
   "source": [
    "## Web Scraper (Example on website)\n",
    "\n",
    "This would be an example of how to create a simple webscraper for a more specialized website, one in which we must use the CSS selectors or XML paths to get specific data. This web scraper employs the use of playwright. The data extracted from these scrapers must then be preproccesed into a Document object for Llama index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c421d-81db-450c-8411-7eb7f666e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of a more specialized web scraper.\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "\n",
    "async def process_locator(locator):\n",
    "    count = await locator.count()\n",
    "    if count > 1: #We have many elements and must resolve each inner text\n",
    "        texts = \"\"\n",
    "        for i in range(count):\n",
    "            element = locator.nth(i)\n",
    "            if await element.is_visible():\n",
    "                inner_text = await element.inner_text()\n",
    "                texts = texts + \",\" + inner_text\n",
    "                return texts\n",
    "    else:\n",
    "        if await locator.is_visible():\n",
    "            return await locator.inner_text()\n",
    "        else:\n",
    "            return \"NA\"\n",
    "    \n",
    "\n",
    "async def main():\n",
    "   async with async_playwright() as pw:\n",
    "       browser = await pw.chromium.launch(\n",
    "           ##We'll employ the use of chromium for this webscraper\n",
    "           ##Using a proxy creates HTTP errors.\n",
    "          headless=False\n",
    "      )\n",
    "\n",
    "       #Beginning page: \n",
    "       page = await browser.new_page()\n",
    "       await page.goto('PAPER_URL')\n",
    "       await page.wait_for_timeout(5000)\n",
    "       result = []\n",
    "       nextPage_urls = []\n",
    "       page_list = await page.query_selector_all('.list_paper_a') #Assuming we have a website of lists of papers\n",
    "       for page in page_list:\n",
    "           nextPage_urls.append(await page.get_attribute('href'))\n",
    "           \n",
    "       for page_url in nextPage_urls:\n",
    "            paper_info = {}\n",
    "            await page.goto(page_url)\n",
    "           ##NOTE: These CSS selectors are highly dependent in the overall structure of the page, and the paper. However, making an assumption \n",
    "           #of some website that has drug information in a somewhat consistent manner we can extract relevant information as follows. \n",
    "            #Title: \n",
    "            title = page.locator(\".title-1\")\n",
    "            paper_info['title'] = await process_locator(title)\n",
    "            #Common Name:\n",
    "            drug_name = page.locator(\"#field_generic_name_value\")\n",
    "            paper_info['drug_name'] = await process_locator(common_name)\n",
    "            #Quantity:\n",
    "            quantity = page.locator(\"#field_quantity_value\")\n",
    "            paper_info['quantity'] = await process_locator(quantity)\n",
    "            #Packaging: \n",
    "            packaging = page.locator(\"#field_packaging_value\")\n",
    "            paper_info['packaging'] = await process_locator(packaging)\n",
    "            #Brands:\n",
    "            brand = page.locator(\"#field_brands_value\")\n",
    "            paper_info['brand'] = await process_locator(brand)\n",
    "            #Categories:\n",
    "            categories = page.locator(\"#field_categories_value\")\n",
    "            paper_info['categories'] = await process_locator(categories)\n",
    "            #Certifications:\n",
    "            certifications = page.locator(\"#field_labels_value\")\n",
    "            paper_info['certifications'] = await process_locator(certifications)\n",
    "            #Origin:\n",
    "            origin = page.locator(\"#field_origin_value\")\n",
    "            paper_info['origin'] = await process_locator(origin)\n",
    "            #origin of ingredients:\n",
    "            origin_of_ingredients = page.locator(\"#field_origins_value\")\n",
    "            paper_info['origin_of_ingredients'] = await process_locator(origin_of_ingredients)\n",
    "            #Places of manufacturing:\n",
    "            places_manufactured = page.locator(\"#field_manufacturing_places_value\")\n",
    "            paper_info['places_manufactured'] = await process_locator(places_manufactured)\n",
    "            #Stores:\n",
    "            stores = page.locator(\"#field_stores_value\")\n",
    "            paper_info['stores'] = await process_locator(stores)\n",
    "        \n",
    "            result.append(food_info)\n",
    "            \n",
    "\n",
    "\n",
    "       \n",
    "       \n",
    "\n",
    "       \n",
    "           \n",
    "           \n",
    "           \n",
    "       await browser.close()\n",
    "       return result\n",
    "if __name__ == '__main__':\n",
    "   result = await main()\n",
    "#CITATIONs: \n",
    "#Code cited from OxyLabs: https://github.com/oxylabs/playwright-web-scraping?tab=readme-ov-file\n",
    "#,https://playwright.dev/python/docs/locat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e565ed3-6891-498d-a917-e8a8c2d27a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concatenated the documents\n",
    "\n",
    "documents = pdf_documents + web_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a21a0-700f-4bb1-92fe-329650c0962c",
   "metadata": {},
   "source": [
    "# Information Indexing & Relevance Filtering \n",
    "\n",
    "For this task, we employ the use of Retrieval Augmented Generation (RAG). Note that I am choosing this over fine-tuning at this moment due to the time constraint. Fine-tuning a specific open source model on Hugging Face, for example, could provide accurate results for a QA system, but is generally more expensive of a procedure. Additionally, fine-tuning provides a more static model, and requires a \"re fine-tuning\" with every new dataset. whereas RAG is more dynamic.\n",
    "\n",
    "RAG's essentially work by creating a data structure (embeddings) of our documents, and allowing our LLM to more or less mathematically provide an answer to our query based on the vector embeddings of both. A particularly good library for RAG is LlamaIndex. Note: At this point we assume to have the pdfs and relevant websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7461a-2969-40a3-8f7a-f306c5b8e356",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "We then take our documents and return vector emebeddings, allowing for a data structure that allows our LLMs to query the data. For this procedure we can employ the use of LLM's to embedded our data, OpenSource (HuggingFace) and OpenAI llms provide this functionality. Notice, in the Installation Instructions I provided a choice for both, this is the case as an API key is required for OpenAI use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e0b6b-d79f-45ef-95f7-253135a24ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI usage: \n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# global\n",
    "Settings.embed_model = OpenAIEmbedding()\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b0ba9-478b-4509-8653-7f8fa71e6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Hugging Face usage (bge-small-en-v1.5):\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\" #The model that we use from Hugging Face provides alot of diversity of choice for one that is best suited for task\n",
    ")\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96e817-752d-4936-80f6-20a32d7cd9fd",
   "metadata": {},
   "source": [
    "## Retriever \n",
    "\n",
    "Retrievers objects allow us to get the most relevant answer given a query. As such, this is a possible method to **Relevance Filtering**. For this, we use the VectorIndexRetriever. (Note: There are many selections and preferences for retriever) The retriever allows for fetching the maximum relevant context for our query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d07165-3eb2-496b-b512-b1debb221c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    ")\n",
    "\n",
    "# configure response synthesizer (We use the default response synthesizer)\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21015cb-1c46-4735-9c33-dc97d38d6103",
   "metadata": {},
   "source": [
    "## Query by LLM\n",
    "\n",
    "Note that this is the simplest implementation, Llama Index provides an extensive ability of control, and as such we can have more \"low-level\" code here. For instance, we can have a streaming response (similar to ChatGPT) that reduces the perceived latency of the response, a user experience choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae7e3d-5bb1-4893-8ae0-973b57c7cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query  \n",
    "response = query_engine.query(\"What are the main competitor drugs of [drug name] and what are their latest news now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d53ce-f676-4a53-b67b-4512446b18d2",
   "metadata": {},
   "source": [
    "## BONUS: Agents\n",
    "\n",
    "We can extend our query engine to act more \"intelligently\" on our data by further calling external services or our own defined tools on the data given. Agent use could be a great addition to the task, as for instance, we can create a better summarization process through the use of specifically created agent tools! \n",
    "\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18424fc2-32f3-4dc0-bd30-3433449cd7b0",
   "metadata": {},
   "source": [
    "## Evaluation (Summarization)\n",
    "\n",
    "Llama provides an extensive library for evaluation of our pipeline. There are a few methods that would be the most beneficial for monitoring the performance of our pipeline. We would like to have our model tested on question by our documents before relying on the answer given for our actual wanted query. To do have this functionality we use llama-index Question Generation. We then use a RelevancyEvaluator, that allows use to evaluate the \"relevancy\" of the answers generated by our model. Below we have an example with gpt-4, again this can be extend towards open source models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c67419-3d48-4949-ad05-ccbf4f87ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suppose we have our document object prepared\n",
    "from llama_index.core.evaluation import DatasetGenerator, RelevancyEvaluator\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Response\n",
    "from llama_index.llms.openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "data_generator = DatasetGenerator.from_documents(documents)\n",
    "eval_questions = data_generator.generate_questions_from_nodes() ## Here we generate questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d9886-37b2-4556-a296-7a647fabd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4\n",
    "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "\n",
    "evaluator_gpt4 = RelevancyEvaluator(llm=gpt4) ## Here we create the Relevancy Object\n",
    "\n",
    "# create vector index\n",
    "vector_index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460f292-0b81-437f-8554-4e698aae720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define jupyter display function, that produces a visual our query and response\n",
    "def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": (\n",
    "                response.source_nodes[0].node.get_content()[:1000] + \"...\"\n",
    "            ),\n",
    "            \"Evaluation Result\": eval_result,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bfc850-d976-4bbd-b181-c43f9d50603a",
   "metadata": {},
   "source": [
    "The cell below produces a dataframe to show the relevancy of our answers. \n",
    "\n",
    "CITED: https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c09454-f2b0-4933-ba10-ea77623ac315",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()\n",
    "response_vector = query_engine.query(eval_questions[1])\n",
    "eval_result = evaluator_gpt4.evaluate_response(\n",
    "    query=eval_questions[1], response=response_vector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098f363-767e-4623-8e85-7e7d0e1aa0d0",
   "metadata": {},
   "source": [
    "NOTICE: The use of the relevancy evaluator would be incredibly useful for an open-source model because if we decided to use fine-tuning with RAG we can then produce distribution of relevancy based on the each fine tune, or each model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59200ed2-f51e-4cd5-ae89-2deecb7fec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can then extend this to a open-source model, and finally evaluate on our query before accepting the answer. \n",
    "# create llm\n",
    "llm = OpenAI(model=\"HUGGING_FACE_MODEL\", temperature=0.0)\n",
    "\n",
    "# build index\n",
    "...\n",
    "\n",
    "# define evaluator\n",
    "evaluator = RelevancyEvaluator(llm=llm)\n",
    "\n",
    "# query index\n",
    "query_engine = vector_index.as_query_engine()\n",
    "query = \"Could you identify the primary competitve drugs to [NAME] and clarify whether they incorporated real-world evidence in their submissions to the FDA or EMA?\"\n",
    "response = query_engine.query(query)\n",
    "response_str = response.response\n",
    "for source_node in response.source_nodes:\n",
    "    eval_result = evaluator.evaluate(\n",
    "        query=query,\n",
    "        response=response_str,\n",
    "        contexts=[source_node.get_content()],\n",
    "    )\n",
    "    print(str(eval_result.passing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff7f84-4aef-4359-acf1-786ba2f10b93",
   "metadata": {},
   "source": [
    "Llama provides a great image for this usage pattern:\n",
    "\n",
    "![title](img/eval_query_response_context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802920a-2d17-4fb1-a801-66f1dbd3bf73",
   "metadata": {},
   "source": [
    "## User Interface Design\n",
    "\n",
    "I would like to pitch the use of Gradio for a simple user-interface. This library gives machine learning models/pipelines a great web interface. This open source python library creates quick web applications for ML models, such that they can be easily shared among the required users. This UI can then be hosted and used within the company easily. \n",
    "\n",
    "CITE: https://www.gradio.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "665a12b8-f074-4189-96f6-05ef20f8d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def process_query(dataset, param1, param2, param3, query):\n",
    "    # Placeholder for processing logic.\n",
    "    # The pipeline that we have created will be called here: \n",
    "    #...\n",
    "    #...\n",
    "    answer = \"Processed answer based on the dataset and parameters would appear here.\"\n",
    "    return answer\n",
    "\n",
    "# Placeholder for the save documentation function\n",
    "def save_documentation(dataset, query, answer):\n",
    "    # Placeholder for saving documentation logic.\n",
    "    # This function should be expanded to actually save the documentation in a persistent store.\n",
    "    print(f\"Documentation saved for dataset: {dataset}, query: {query}, answer: {answer}\")\n",
    "    return \"Documentation has been saved.\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"<h1>Paper Query</h1>\")  # Title for the UI\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            dataset = gr.Dropdown([\"Paper Documentation 1\", \"Paper Documentation 2\", \"Paper Documentation 3\"], label=\"Dataset\")\n",
    "            param1 = gr.Checkbox(label=\"Parameter 1\")\n",
    "            param2 = gr.Checkbox(label=\"Parameter 2\")\n",
    "            param3 = gr.Checkbox(label=\"Parameter 3\")\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            save_button = gr.Button(\"Save this documentation\")  # Button for saving documentation\n",
    "        with gr.Column():\n",
    "            answer = gr.Textbox(label=\"Answer\", lines=10, placeholder=\"Your answer will appear here...\")\n",
    "            feedback_good = gr.Button(\"Good Answer\")\n",
    "            feedback_bad = gr.Button(\"Bad Answer\")\n",
    "            feedback_response = gr.Label()  # To display a response after feedback is submitted\n",
    "\n",
    "    query = gr.Textbox(label=\"Query\", placeholder=\"Type your query here...\")\n",
    "    \n",
    "    # # Function bindings\n",
    "    # submit_button.click(\n",
    "    #     fn=process_query,\n",
    "    #     inputs=[dataset, param1, param2, param3, query],\n",
    "    #     outputs=[answer]\n",
    "    # )\n",
    "    \n",
    "    # save_button.click(\n",
    "    #     fn=save_documentation,\n",
    "    #     inputs=[dataset, query, answer],\n",
    "    #     outputs=[]\n",
    "    # )\n",
    "\n",
    "    # feedback_good.click(\n",
    "    #     fn=lambda is_good, answer: \"Thank you for your feedback!\",\n",
    "    #     inputs=[True, answer],\n",
    "    #     outputs=[feedback_response]\n",
    "    # )\n",
    "    \n",
    "    # feedback_bad.click(\n",
    "    #     fn=lambda is_good, answer: \"Thank you for your feedback!\",\n",
    "    #     inputs=[False, answer],\n",
    "    #     outputs=[feedback_response]\n",
    "    # )\n",
    "\n",
    "# Launch the app\n",
    "demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ee808-5c45-48a5-b5bd-756bf042b6f8",
   "metadata": {},
   "source": [
    "![title](img/basic_rag.png)\n",
    "\n",
    "\n",
    "High-level RAG image cited from Llama Index: https://docs.llamaindex.ai/en/stable/getting_started/concepts/#indexing-stage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
